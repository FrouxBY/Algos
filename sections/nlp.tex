%!TEX root = ../master.tex
\chapter{NLP} % (fold)
\label{cha:nlp}
		
		Thanks to Lucia Specia Course at Imperial Collage and several ressources : \par
		\url{https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf} \par
		\url{https://web.stanford.edu/~jurafsky/slp3/}
		\section{What is NLP}

			Natural language Processing (or Natural Language Understanding sometimes) is the field which try to make machine understand natural human language. Indeed we have developped really complex way of communication thanks to vocaculary, grammar, syntax and semanticDomain a the intersection between AI, Computer Sciences, Linguistics and Cognitive Science.

			The goal are to undersand NL, execute requested task and produce back NL as answer. The field generally focus on written language as speech and handwritten require only speech-to-text and OCR to be computed with NLP models. 

			\subsection{Linguistic required}

				Working on NLP require some knowledge of linguistics in order to understand the different problematics and the possible solutions
				\begin{itemize}
					\item  a word
					\item Word Segmentation (tokenization and decompounding)
					\item Lemmatization
					\item Morphological analysis (POS Tagging, Suffixes (unhappy-happy), gender, number and tense)
					\item Grammar (phrase-structure)
					\item Semantics (Lexical meaning and compositionnal meaning)
					\item References and relationship
				\end{itemize}

			\subsection{NLP Application}
				This are different kind of applications of NLP models :
				\begin{itemize}
					\item Sentiment analysis
					\item Fake News/Deception detection
					\item Offensive Language detection and categorisation
					\item Question answering
					\item Machine translation
					\item Text summarisation
					\item Dialog System (Phone assistant)
				\end{itemize}

	\section{How to represent word}

		In order to use maths to infer anything on word, the first questions that raised is "How to represent words in a mathematical way ?". This can be rephrased as a question about encoding the meaning of a word. The definitions of the meaning of a word have been studied for a long time by philosophy and linguistic, and this can inspire maths a lot. 

		The notions of lexical semantics (meaning of a word) cover those following subjects :  Lemma and Senses, Relationship between words or senses (Propositionnal meaning, Principle of Contrast, Antonyms/reversive), Word similarity and Word relatedness (Co-participating in shared events), Taxonomic relations.

		Thanks to this notion, we have a better insight of the meaning of words, however, this does not come with any computational model. The current best models are \emph{Vector semantics} models.

		The following paragraphs show different methods or general concept to encode words (on a sentence level or word level). Usually the objective is to find a vector representation of the entities we want to describe.

		\paragraph{Bag of Word}
			Count the number of word of in a sentence, this gives us a One Hot Vector (can be normalized as a frequency vector) as input.\\
			Problem : 
			\begin{itemize}
				\item Very Sparse (if each word of the dictionnary is a dimension, this make more than 170 000 dimension for english)
				\item This does not give any intel about the relationship of words - of common subject (e.g. Cat and kitten) or as grammar (subject, verb, etc.)
			\end{itemize}

		\paragraph{Mapping with concept}
			Use logical concept to classify words : \\
			-> Cat and kitten are feline mammal \\
			-> Mouse and rat are rodent mammal\\
			The WordNet (https://en.wikipedia.org/wiki/WordNet) synset (dictionnary of synonymes and themes) group words in such a way and is a reference for AI and ML application and research.
			Problem :
			\begin{itemize}
				\item Misses Nuances between words as concept are based on similiraty and not on differences.
				\item Misses new meanings, languages evolutions, etc. 
				\item Classify word as concept without those issues is a problem in itself called Word Sense Disambiguation (WSD).
				\item Ressources does not exist for most languages or even for specialised vocabulary (Medical, any technical subject).
			\end{itemize}


		\paragraph{Features extraction}
			An other solution is to extract only some features from words. Those may have to be designed depending on the application (for better accuracy), but this can be powerful and efficient. For exemple counting the \% of positive and negative words for sentiment analysis. 
			Problem : 
			\begin{itemize}
				\item Require domain knowledge
				\item The feature are really task-specific and the extraction can be difficult.
			\end{itemize}

		\paragraph{Corpora and distributional hypothesis}
			All those representations forget (partially or totally) a significant aspect of Natural Language, the context. A way to explain and describe context is to say that a word meaning is given by the words arround it, then the precise meaning of a word can be 'capture' in the word itself plus probability of surrounding word beeing in a specific corpora. \\

			With a Fixed size of window context and a many corpora with different context available this is computable. \\
			The use of probabilities and frequencies help us to move from discrete to continous representations, which makes possible to use a wider variety of model, which achieve generally better results. 

			This way (Describe by Firth in 1957) gives some explanations for why further methods work.

			It can be done with different manner, for example with Co-occurence matrix. This are also One Hot vector, but here each word is a vector and not each sentences. Different meaning of the same word can be encoded as different vector, it is easy to remove stop words (common word use in natural languages, but with a low ammount of meaning), Terms can be weighted by TF-IDF (term frequencyâ€“inverse document frequency) and some notion of similarity ca be incorporated (distance between vector for example). All of this tend to escape the discrete representation, however this is still very large space and therefore very sparse representations.

		\paragraph*{TF-IDF : Weighing terms in the vector}

			Co-occurence matrix take the raw frequency of co-occurent words, but this is not the best measure of association between words. The raw frequency is not enough discriminative as a lot of really frequent word but meaningless can be shared by two words.

			In order to avoid this paradox, the TF-IDF (Term frequency - Inverse document frequency) use the product of TF $= 1 + \log_{10} (count(t, d) $ (or 0) and the IDF $= \log_{10} \frac{N}{df_t}$ where $df_t$  is the number of document where $t$ occures and $N$ the number of all document. 

			\[
				\texx{tf-idf} = tf_{t,d} \time iff_t
			\]

			The tf-idf is commonly use as a baseline for weighting a co-occurence Matrix.
		\paragraph*{Vector Space models}

			In order to use known Inference/Machine Learning models, we want to represent words as vector in mathematical space. We embed words in a continuous vector space where semantically similar words are mapped to nearby points. The previous description (based on Co-Occurence matrix and diverse weighting) is not enough, but the use of Dimension reduction can help us to get more dense representation. For example the use of SVD (Singular Value Decomposition) can easily reduce the dimension of 170 000 words to 1 000 dimensions. \\
			Problems:
			\begin{itemize}
				\item SVD is computationnally expensive ($O(mn^2)$)
				\item Incorporating new word require to re-compute the SVD
			\end{itemize}

			The idea to solve those two problems have been found in representation learning, and their efficiency are the explanation of most of the progress in NLP in the recent years.


	\section{Learning word representation}

		The solution to most of the word representation problems is the learning of those representations. This work as many machine learning problem, the model test how well it performs on a task and the error is used to enhance the representation. \\
		Here the task is the prediction of context words in context. For a given word as input the model gives a predicted context (words) as output, the score is given by a distance between words (using corpora as training set). Of course each word are vector. This is both computationnaly efficient (compared to SVD) and scalable with the corpus size (to some extent).\\
		All deep learning Idea can then be applied here, with more or less efficiency.

		\subsection{Word embeddings : word2vec}

			The most known word embeddings modelx (which have been a revolution in the NLP field) are word2vec (by Mikolov et al., 2013). Those papers \todo{Add ref to Mikolov papers} introduce two different models :
			\begin{itemize}
				\item Continous Bag-of-Words model (CBOW):\\
				- The objective is to predict a target word $w_t$ given context words $w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}$.
				\item Skip-Gram model:\\
				- The objective is to predicts context words  $w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}$ given a target word $w_t$.
			\end{itemize}

			But actually we are not really interested in the prediction made by the algorithms, but only in the weights of the models, which give us a dense representation of the words. Therefore it can be rephrased as a classification problem : given two words, how likely they can occure together. This can be done with simple Logistic Regression classifier, and the weight of the logistic regression will be  the embedding matrix. 
  
		\subsection*{Skip-gram model}

			The Goal of the Skip-Gram model is to find word representations that are useful for predicting context word ($w_{t+j}$) given a word $w_t$. For this we maximise the following equation : 
			\[
				\frac{1}{T}\sum^T_{t=1} \sum_{-c\leq j\leq c}\log p(w_{t+j} | w_t, \theta)
			\]
			which is the average log probabilities for context window $c$ given the parameters $\theta$. $T$ is the size of the corpus.\\
			The probability can be defined using the softmax function : 
			\[
				p(w_O |  w_I) = \frac{\exp(v_{w_O}\T v_{w_I})}{\sum_{w\in W}\exp(v_{w}\T v_{w_I})}
			\]
			where $v_w$ are the embedding vector of the word $w$

			\begin{itemize}
				\item 1. Treat the 
			\end{itemize}

			\paragraph*{Neural Net implementation}

			The neural network for the Skip gram model is simple, the input vectors are one-hot vector of the size of the language (Ex: 10 000), then there is a hidden layer of significantly fewer dimension (Ex: 300), which are fully connected to an output layer of the size of the language (here 10 000) with a softmax activation. Each dimension $i$of the output learn the probability of having the word $w_i$ as a context word for the input $w_I$. After learning, we can remove the output layer, and getting the embedding of the words as the output of the previous layer. This can be seen a little bit as encoder neural net for computer vision, with encoder and decoder part. 

			As a result, the hidden layer is a embedding matrix, working as a look-up table. 

			The training can be done with Gradient Descent and Neural net algorithm for learning. How ever, there are a lot of parameters in naive Skip-Gram (size of corpus time embedding dimension) which are prohibitively expensive for large dataset.

			\paragraph*{Negative Sampling}
				However there is no need for a full probabilistic model and some trick can help to reduce the computation. A binary classification objectif can be used for training the model, using logistic regression to discriminate real target words from other noise words. This is called Negative Sampling, which can be written as maximising : 
				\[
					\log Q_\theta (D=1| w_t, c) + k \mathbb{E}_{w_\sim P_{noise}}[log Q_\theta(D=0 | w, c)]
				\]
				Where the first term is the binary logistic regression probability of seeing the word $w_t$ in the context $c$ in the dataset $D$, calculated in terms of the learned embedding vectors. The expectation is approximated bu using $k$ contrastive words from a noise distribution.

				As for intuition, this can be seen as the following : \\
				- To predict a word Context from a word Input, first select k noisy (i.e. contrastive) words which are not in the context of the word Input and then compute the loss for observed and contrastive pairs. at a step $t$, the objective is $ \log Q_\theta(D=1 | Context, Input) + \log Q_\theta(D=0 | Noise_k, Input)$

				How to select the Negative Sampling ? The probability for selecting a word need to be related to its frequency. The number of sample required need to be larger for small dataset and smaller for large datasets. Arround 5-20 words against 2-5 words for larger dataset. 

			\paragraph*{Exemple of vector representation}
				Add explanation about what vector capture (king, queen, man, woman exemple)
		
			\paragraph*{Other tricks}
				\begin{itemize}
					\item Treating common words pairs as single "words"
					\item Subsampling frequent words to decrease the number of training examples
					\item Using hierachical softmax
				\end{itemize}

			\paragraph*{Other kind of embeddings word}
				CBOW, Glove, ELMo and BERT
			Contextualised word embeddings seems really powerful, but really costly

			\subsection{Discussion on Embedddings}

				\paragraph*{Bias and Embeddings}
					One of the problem with embeddings is that the learning process relies on a dataset (the corpus) which is by nature imperfect and biaised. One Exemple is the gender biais : As the corpus contains data about job, it may decide that man and woman have different similarity for job and reproduce that nurse is associated with women and doctor to man as this is the bias of the corpus (Bolukbasi et al. 2016).

				\paragraph*{Evaluating Vector Models}
				\begin{itemize}
				  	\item Test performance on similarity
				  	\item WordSim-353
				  	\item SimLex-999
				  	\item Toefl Dataset
				  	\item Stanford Contextual Word Similarity (SWCS)
				  	\item Analogy task
				  \end{itemize}  


	\section{Some Algorithm for NLP}

		NLP use classifier for different tasks, such as sentiment analysis, fake news, authorship  attribution, detection of suicide risk, patient risk assessment, etc.

		\subsection{Naives Bayes Classifier}

			Used a lot in anti-spam algorithm

			Work with a training corpus, use feature vector of word input $x = [x_1, ..., x_I]$
			and we are looking for the output class $\hat y = \arg \max_y P(y|x) = \arg \max_y P(x | y)P(y)$

			We assume in Naive Bayes that the features of $x$ are independant. Which makes possible to write : 
			\[
				\hat y = \arg \max_y P(y) \Pi_1^I P(x_i | y)
			\]
			which lead in practise to
			\[
				P(y) = \frac{N_y}{N_x} \text{ and } P(x_i | y) = \frac{\text{count}(x_i, y)+1}{\Sigma_{x\in V}(\text{count(x, y)+1})}
			\] with $V$ the vocabulary across classes

			\paragraph*{Problems}

				\begin{itemize}
					\item All featurees are equally important (this is not the case generally)
					\item There is a strong conditonal independence assumption
					\item The context is not taken into account
					\item Unknown words are not tractable
				\end{itemize}

		\subsection{Logistic Regression}
			See Chapter Regression for details, basically we are adding weight to each features $x_i$ of $x$ \\
			example : The Movie is Bad $\rightarrow$ Bad might be more important than movie in sentiment analysis.

			\paragraph*{Problems}
				
		\subsection{Neurals Networks}
			See DeepLearning for details of what is a neural networks. \\

			The imput of neural networks can be one-hot representation of words, automatically learn dense feature representations (word embedding within the NN) or pre-trained dense representation (from another models)

			NN are used because of the automatic learning of features which are hard to describe humanly (because of grammar, semantics, etc) and the non-linearity, their flexibility to fit highly complex data. 

			But their require lot of data for training. 


		\subsection{CNNs in NLP}

			\subsubsection{Context approach}

			\subsubsection*{Character-Level Approach}

		\subsection{Recurrent Neural Networks (RNN)}
			Use the sequence structure of language. Very efficient but none paralelizable. 
		\section{Preprocessing in NLP}
			Require consistency, truecase (instead of lowercase), removal of stopwords (espacially important for smaller datasets)
		\section{Evaluation of NLP model}
			3 different possible evaluations :
			\begin{itemize}
				\item Precision : $\frac{TP}{TP+FP}$
				\item Recall : $\frac{TP}{TP+FN}$
				\item Accuracy $\frac{TP+TN}{TP+FP+TN+FN}$
				\item F-Mesure: $F = k \times \frac{precision \times recall}{precision + recall}$
			\end{itemize}
			With TP = True positives, FP = False positives, TN = True negatives, FN= False negatives

		\section{Language Model}
			Language model task = LM's tasks.
			Language models that assign a probability to each possible next word given a history of words. This can be generalised for entire sentences or operate at character level (or any symbol)

			\subsection{Basic N-gram language models}
				A N-grams in simply a sequence of $n$ words, the LM's task here is to evaluate $P(w|h)$ where $w$ is next word and $h$ the history. P can then be estimated from frequency counts, but require large corpus.\\
				Longer history or even for full sentences require very large corpus, and is not feasible for long sentences.\\
				However, we can decompose the n-gram as n different word. This is translate in term of probablility by $P(h) = P(w_1 ... w_n) = P(w_1)P(w_2 | w_1)...P(w_n | w_1 ... w_{n-1})$ using a conditional chain rule. This help us to see this as Markov Chain, and making Markov Assumption : \\
				\[
					P(w_n | w_1^{n-1}) \simeq P(w_n | w_{n-N+1}^{n-1})
				\]
				where $w_1^{n-1} = w_1 ... w_{n-1}$. This mean that for a n-gram, the next word probability depend only the last $N$ few words (or are approximate so).

				\begin{itemize}
					\item MLE as relative frequencies
					\item Corpus Size
					\item Technicalities
					\item Use log space
					\item How to decide on n (of the n-gram)
					\item Google exemple ressource
				\end{itemize}
			\subsection{Evaluation of language models}
				- Does it prefer real sentences to ungrammatical ones ?\\
				- Measure the perplexity of an unseen corpus.\\
				- Perplexity it the inverse probability of the test set. 
			\subsection{Sparsity}

		\section{Structured prediction}

			\subsection{Part-Of-Speech tagging (POS)}

				\paragraph*{Why do we need POS tagging}

				\paragraph*{tagset}

				\paragraph*{baseline method}


				\paragraph*{POS ambiguities}
			\subsection{Probabilistic POS tagging}

			\subsection{Hidden Markov Model (HMM) tagger}

			\subsection{Maximum Entropy Markov Model (MEMM) for POS tagging}

			\subsection{Other approachs}
				CRF

			\paragraph*{Performance}
				MEMM get 97\% against 98\% for human

			\paragraph*{Tools}
				Spacy, NLTK

		\section{Sequence Models}

			\paragraph*{Sequence Tagging}

			\subsection{Types of Sequence Models}

			\subsection{Neural Sequence Modelling}

				\paragraph*{Non-Markovian Assumption}

			\subsection{Language Modelling with Recursion}

		\section{Recurrent Neural Network}
			\subsection{Structure of LM}

			\subsection{Training of RNNLM}

				\paragraph*{Effective Back-Prop Strategies}

					-Back-Prop Through Time\\
					-Truncated Back-Prop Through Time

				\paragraph*{Vanishing Gradient}

			\subsection{Gated Recurrent Neural Network}

			\subsection{Other Variant}

				- Long Short-Term Memory networks (LSTM)
				- Similar functions
		\section{Bi-directional RNN}

		\section{Transformer Based Models}





		
% chapter nlp (end)