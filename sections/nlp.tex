%!TEX root = ../master.tex
\chapter{NLP} % (fold)
\label{cha:nlp}
		
	Thanks to Lucia Specia Course at Imperial Collage and several ressources : \par
	\url{https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf} \par
	\url{https://web.stanford.edu/~jurafsky/slp3/}
	
	\section{What is NLP}

			Natural language Processing (or Natural Language Understanding sometimes) is the field which try to make machine understand natural human language. Indeed we have developped really complex way of communication thanks to vocaculary, grammar, syntax and semanticDomain a the intersection between AI, Computer Sciences, Linguistics and Cognitive Science.

			The goal are to undersand NL, execute requested task and produce back NL as answer. The field generally focus on written language as speech and handwritten require only speech-to-text and OCR to be computed with NLP models. 

			\subsection{Linguistic required}

				Working on NLP require some knowledge of linguistics in order to understand the different problematics and the possible solutions
				\begin{itemize}
					\item  a word
					\item Word Segmentation (tokenization and decompounding)
					\item Lemmatization
					\item Morphological analysis (POS Tagging, Suffixes (unhappy-happy), gender, number and tense)
					\item Grammar (phrase-structure)
					\item Semantics (Lexical meaning and compositionnal meaning)
					\item References and relationship
				\end{itemize}

			\subsection{NLP Application}
				This are different kind of applications of NLP models :
				\begin{itemize}
					\item Sentiment analysis
					\item Fake News/Deception detection
					\item Offensive Language detection and categorisation
					\item Question answering
					\item Machine translation
					\item Text summarisation
					\item Dialog System (Phone assistant)
				\end{itemize}

	\section{How to represent word}

		In order to use maths to infer anything on word, the first questions that raised is "How to represent words in a mathematical way ?". This can be rephrased as a question about encoding the meaning of a word. The definitions of the meaning of a word have been studied for a long time by philosophy and linguistic, and this can inspire maths a lot. 

		The notions of lexical semantics (meaning of a word) cover those following subjects :  Lemma and Senses, Relationship between words or senses (Propositionnal meaning, Principle of Contrast, Antonyms/reversive), Word similarity and Word relatedness (Co-participating in shared events), Taxonomic relations.

		Thanks to this notion, we have a better insight of the meaning of words, however, this does not come with any computational model. The current best models are \emph{Vector semantics} models.

		The following paragraphs show different methods or general concept to encode words (on a sentence level or word level). Usually the objective is to find a vector representation of the entities we want to describe.

		\paragraph{Bag of Word}
			Count the number of word of in a sentence, this gives us a One Hot Vector (can be normalized as a frequency vector) as input.\\
			Problem : 
			\begin{itemize}
				\item Very Sparse (if each word of the dictionnary is a dimension, this make more than 170 000 dimension for english)
				\item This does not give any intel about the relationship of words - of common subject (e.g. Cat and kitten) or as grammar (subject, verb, etc.)
			\end{itemize}

		\paragraph{Mapping with concept}
			Use logical concept to classify words : \\
			-> Cat and kitten are feline mammal \\
			-> Mouse and rat are rodent mammal\\
			The WordNet (https://en.wikipedia.org/wiki/WordNet) synset (dictionnary of synonymes and themes) group words in such a way and is a reference for AI and ML application and research.
			Problem :
			\begin{itemize}
				\item Misses Nuances between words as concept are based on similiraty and not on differences.
				\item Misses new meanings, languages evolutions, etc. 
				\item Classify word as concept without those issues is a problem in itself called Word Sense Disambiguation (WSD).
				\item Ressources does not exist for most languages or even for specialised vocabulary (Medical, any technical subject).
			\end{itemize}


		\paragraph{Features extraction}
			An other solution is to extract only some features from words. Those may have to be designed depending on the application (for better accuracy), but this can be powerful and efficient. For exemple counting the \% of positive and negative words for sentiment analysis. 
			Problem : 
			\begin{itemize}
				\item Require domain knowledge
				\item The feature are really task-specific and the extraction can be difficult.
			\end{itemize}

		\paragraph{Corpora and distributional hypothesis}
			All those representations forget (partially or totally) a significant aspect of Natural Language, the context. A way to explain and describe context is to say that a word meaning is given by the words arround it, then the precise meaning of a word can be 'capture' in the word itself plus probability of surrounding word beeing in a specific corpora. \\

			With a Fixed size of window context and a many corpora with different context available this is computable. \\
			The use of probabilities and frequencies help us to move from discrete to continous representations, which makes possible to use a wider variety of model, which achieve generally better results. 

			This way (Describe by Firth in 1957) gives some explanations for why further methods work.

			It can be done with different manner, for example with Co-occurence matrix. This are also One Hot vector, but here each word is a vector and not each sentences. Different meaning of the same word can be encoded as different vector, it is easy to remove stop words (common word use in natural languages, but with a low ammount of meaning), Terms can be weighted by TF-IDF (term frequencyâ€“inverse document frequency) and some notion of similarity ca be incorporated (distance between vector for example). All of this tend to escape the discrete representation, however this is still very large space and therefore very sparse representations.

		\paragraph*{TF-IDF : Weighing terms in the vector}

			Co-occurence matrix take the raw frequency of co-occurent words, but this is not the best measure of association between words. The raw frequency is not enough discriminative as a lot of really frequent word but meaningless can be shared by two words.

			In order to avoid this paradox, the TF-IDF (Term frequency - Inverse document frequency) use the product of TF $= 1 + \log_{10} (count(t, d) $ (or 0) and the IDF $= \log_{10} \frac{N}{df_t}$ where $df_t$  is the number of document where $t$ occures and $N$ the number of all document. 

			\[
				\text{tf-idf} = tf_{t,d} \times iff_t
			\]

			The tf-idf is commonly use as a baseline for weighting a co-occurence Matrix.
		\paragraph*{Vector Space models}

			In order to use known Inference/Machine Learning models, we want to represent words as vector in mathematical space. We embed words in a continuous vector space where semantically similar words are mapped to nearby points. The previous description (based on Co-Occurence matrix and diverse weighting) is not enough, but the use of Dimension reduction can help us to get more dense representation. For example the use of SVD (Singular Value Decomposition) can easily reduce the dimension of 170 000 words to 1 000 dimensions. \\
			Problems:
			\begin{itemize}
				\item SVD is computationnally expensive ($O(mn^2)$)
				\item Incorporating new word require to re-compute the SVD
			\end{itemize}

			The idea to solve those two problems have been found in representation learning, and their efficiency are the explanation of most of the progress in NLP in the recent years.


	\section{Learning word representation}

		The solution to most of the word representation problems is the learning of those representations. This work as many machine learning problem, the model test how well it performs on a task and the error is used to enhance the representation. \\
		Here the task is the prediction of context words in context. For a given word as input the model gives a predicted context (words) as output, the score is given by a distance between words (using corpora as training set). Of course each word are vector. This is both computationnaly efficient (compared to SVD) and scalable with the corpus size (to some extent).\\
		All deep learning Idea can then be applied here, with more or less efficiency.

		\subsection{Word embeddings : word2vec}

			The most known word embeddings modelx (which have been a revolution in the NLP field) are word2vec (by Mikolov et al., 2013). Those papers \todo{Add ref to Mikolov papers} introduce two different models :
			\begin{itemize}
				\item Continous Bag-of-Words model (CBOW):\\
				- The objective is to predict a target word $w_t$ given context words $w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}$.
				\item Skip-Gram model:\\
				- The objective is to predicts context words  $w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}$ given a target word $w_t$.
			\end{itemize}

			But actually we are not really interested in the prediction made by the algorithms, but only in the weights of the models, which give us a dense representation of the words. Therefore it can be rephrased as a classification problem : given two words, how likely they can occure together. This can be done with simple Logistic Regression classifier, and the weight of the logistic regression will be  the embedding matrix. 
  
		\subsection*{Skip-gram model}

			The Goal of the Skip-Gram model is to find word representations that are useful for predicting context word ($w_{t+j}$) given a word $w_t$. For this we maximise the following equation : 
			\[
				\frac{1}{T}\sum^T_{t=1} \sum_{-c\leq j\leq c}\log p(w_{t+j} | w_t, \theta)
			\]
			which is the average log probabilities for context window $c$ given the parameters $\theta$. $T$ is the size of the corpus.\\
			The probability can be defined using the softmax function : 
			\[
				p(w_O |  w_I) = \frac{\exp(v_{w_O}\T v_{w_I})}{\sum_{w\in W}\exp(v_{w}\T v_{w_I})}
			\]
			where $v_w$ are the embedding vector of the word $w$

			\begin{itemize}
				\item 1. Treat the 
			\end{itemize}

			\paragraph*{Neural Net implementation}

			The neural network for the Skip gram model is simple, the input vectors are one-hot vector of the size of the language (Ex: 10 000), then there is a hidden layer of significantly fewer dimension (Ex: 300), which are fully connected to an output layer of the size of the language (here 10 000) with a softmax activation. Each dimension $i$of the output learn the probability of having the word $w_i$ as a context word for the input $w_I$. After learning, we can remove the output layer, and getting the embedding of the words as the output of the previous layer. This can be seen a little bit as encoder neural net for computer vision, with encoder and decoder part. 

			As a result, the hidden layer is a embedding matrix, working as a look-up table. 

			The training can be done with Gradient Descent and Neural net algorithm for learning. How ever, there are a lot of parameters in naive Skip-Gram (size of corpus time embedding dimension) which are prohibitively expensive for large dataset.

			\paragraph*{Negative Sampling}
				However there is no need for a full probabilistic model and some trick can help to reduce the computation. A binary classification objectif can be used for training the model, using logistic regression to discriminate real target words from other noise words. This is called Negative Sampling, which can be written as maximising : 
				\[
					\log Q_\theta (D=1| w_t, c) + k \mathbb{E}_{w_\sim P_{noise}}[log Q_\theta(D=0 | w, c)]
				\]
				Where the first term is the binary logistic regression probability of seeing the word $w_t$ in the context $c$ in the dataset $D$, calculated in terms of the learned embedding vectors. The expectation is approximated bu using $k$ contrastive words from a noise distribution.

				As for intuition, this can be seen as the following : \\
				- To predict a word Context from a word Input, first select k noisy (i.e. contrastive) words which are not in the context of the word Input and then compute the loss for observed and contrastive pairs. at a step $t$, the objective is $ \log Q_\theta(D=1 | Context, Input) + \log Q_\theta(D=0 | Noise_k, Input)$

				How to select the Negative Sampling ? The probability for selecting a word need to be related to its frequency. The number of sample required need to be larger for small dataset and smaller for large datasets. Arround 5-20 words against 2-5 words for larger dataset. 

			\paragraph*{Exemple of vector representation}
				Add explanation about what vector capture (king, queen, man, woman exemple)
		
			\paragraph*{Other tricks}
				\begin{itemize}
					\item Treating common words pairs as single "words"
					\item Subsampling frequent words to decrease the number of training examples
					\item Using hierachical softmax
				\end{itemize}

			\paragraph*{Other kind of embeddings word}
				CBOW, Glove, ELMo and BERT
			Contextualised word embeddings seems really powerful, but really costly

			\subsection{Discussion on Embedddings}

				\paragraph*{Bias and Embeddings}
					One of the problem with embeddings is that the learning process relies on a dataset (the corpus) which is by nature imperfect and biaised. One Exemple is the gender biais : As the corpus contains data about job, it may decide that man and woman have different similarity for job and reproduce that nurse is associated with women and doctor to man as this is the bias of the corpus (Bolukbasi et al. 2016).

				\paragraph*{Evaluating Vector Models}
				\begin{itemize}
				  	\item Test performance on similarity
				  	\item WordSim-353
				  	\item SimLex-999
				  	\item Toefl Dataset
				  	\item Stanford Contextual Word Similarity (SWCS)
				  	\item Analogy task
				  \end{itemize}  


	\section{ML Algorithms for NLP}

		NLP use classifier for different tasks, such as sentiment analysis, fake news, authorship  attribution, detection of suicide risk, patient risk assessment, etc.

		\subsection{Naives Bayes Classifier}

			This is the most simple classifier which work well in some NLP application, such as text categorization, it is used a lot in anti-spam algorithm for exemple. 

			Work with a training corpus, use feature vector of word input $x = [x_1, ..., x_I]$
			and we are looking for the output class $\hat y = \arg \max_y P(y|x) = \arg \max_y P(x | y)P(y)$

			We assume in Naive Bayes that the features of $x$ are independant (in particular the order and the position in the text is not taken into account). Which makes possible to write : 
			\[
				\hat y = \arg \max_y P(y) \Pi_1^I P(x_i | y)
			\]
			which lead in practise to
			\[
				P(y) = \frac{N_y}{N_x} \text{ and } P(x_i | y) = \frac{\text{count}(x_i, y)+1}{\Sigma_{x\in V}(\text{count(x, y)+1})}
			\] with $V$ the vocabulary across classes

			In order to ensure the hypothesis, usualy a simple Bag-of-Word representation is used.

			\paragraph*{Problems}

				\begin{itemize}
					\item All featurees are equally important (this is not the case generally)
					\item There is a strong conditonal independence assumption
					\item The context is not taken into account
					\item The MLE training have problem to compute probabilities of word which never appear in some classes. The solution might be to add a Laplace Smoothing.
					\item Unknown words are not tractable (Require to be ignored for predictions)
				\end{itemize}

			\begin{algorithm}[H]
				\KwData{Corpus $D$ and Classes $C$ }
				\KwResult{$\log P(C) and \log P(w|c)$}

				
					\ForAll{$c \in C$}
					{
						$N_{doc} =$ Number of document in $D$\\
						$N_c = $ Number of document within $c$\\
						logprior[c] $\leftarrow \log \frac{N_c}{N_{doc}}$\\
						$V \leftarrow$ Vocabulary of $D$\\
						bigdoc $\leftarrow$ append(d) for d in D with class c\\
						\ForAll{$w \in V$}
						{
							count(w,c) $\leftarrow$ number of occurences of w in bigdoc[c]\\
							loglikelihood[w, c] $\leftarrow \log \frac{count(w,c) +1}{\sum (count(w', c)+1)}$   
						} 
					}
				return logprior, loglikelihood, V
				
				\caption{Naive Bayes Algorithm training with Laplace Smoothing}
			\end{algorithm}


			\subsection{Variant of Bayes Naive}
				- Binary multinomial naive Bayes.\\
				- Use of sentiment lexicons for unlabelled corpus (General Inquirer, LIWC, MPQA, etc.)
		\subsection{Logistic Regression}

			See Chapter Regression for details, basically we are adding weight to each features $x_i$ of $x$ \\
			example : The Movie is Bad $\rightarrow$ Bad might be more important than movie in sentiment analysis.

			The logistic Regression is particularly suitable for discovering link between features and some outcome. This is an other baseline for classification tasks. 

			\paragraph*{Difference with Naive Bayes and advantages}
			The main difference with naive Bayes is that Logistic Regression is a discriminative classifier instead of a Generative Classifier.

			The strong independance assumption of naive Bayes implies that strong correlatio will artificially augment the importance of some features (imagine what happen if you duplicate one feature). Logistic Regression is much more robust to correlated features and will be the preffered task for larger documents and datasets.

			However, on very small datasets or short document Naive Bayes work extremely well and is much more faster to implement (there is no optimization step).
				
		\subsection{Neurals Networks}
			See DeepLearning for details of what is a neural networks. \\

			The imput of neural networks can be one-hot representation of words, automatically learn dense feature representations (word embedding within the NN) or pre-trained dense representation (from another models)

			NN are used because of the automatic learning of features which are hard to describe humanly (because of grammar, semantics, etc) and the non-linearity, their flexibility to fit highly complex data. 

			We talk about Neural Language Models for model which use neural nets to compute the probability of the next word given n previous word

			Neural Language model can use pre-trained word embeddings, making possible to get dense embedding even with small corpus, and to learn faster.

			However Neural Nets require usually lot of data for training correctly (with overfitting, etc). 


		\subsection{CNNs in NLP}

			\subsubsection{Context approach}

			\subsubsection*{Character-Level Approach}

		\subsection{Recurrent Neural Networks (RNN)}
			
			Use the sequence structure of language. Very efficient but none paralelizable. 

	\section{Preprocessing in NLP}

			Require consistency, truecase (instead of lowercase), removal of stopwords (espacially important for smaller datasets)

	\section{Evaluation of NLP model}
			3 different possible evaluations :
			\begin{itemize}
				\item Precision : $\frac{TP}{TP+FP}$
				\item Recall : $\frac{TP}{TP+FN}$
				\item Accuracy $\frac{TP+TN}{TP+FP+TN+FN}$
				\item F-Mesure: $F = k \times \frac{precision \times recall}{precision + recall}$
			\end{itemize}
			With TP = True positives, FP = False positives, TN = True negatives, FN= False negatives

	\section{Language Model}

		Language model task = LM's tasks.
		Language models are models that assign probabilities to each possible next word given a history of words. This can be generalised for entire sentences or operate at character level (or any symbol).

		Why are Language Models useful ? 
		\begin{itemize}
			\item Speech/Handwritten recognition
			\item Spelling correction
			\item Machine Translation
			\item Augmentative Communication
			\item and even more
		\end{itemize}

		In all this task, probability of the next words (or entities) can drastically remove the space of search. 

		\emph{Warning} In the following, n-gram can design either a sequence of n-words or the Language model based on n-gram.

		\subsection{Basic N-gram language models}

				A N-grams in simply a sequence of $n$ words, the LM's task here is to evaluate $P(w|h)$ where $w$ is next word and $h$ the history. P can then be estimated from frequency counts, but require large corpus.\\
				Longer history or even for full sentences require very large corpus, and is not feasible for long sentences.\\
				However, we can decompose the n-gram as n different word. This is translate in term of probablility by $P(h) = P(w_1 ... w_n) = P(w_1)P(w_2 | w_1)...P(w_n | w_1 ... w_{n-1})$ using a conditional chain rule. This help us to see this as Markov Chain, and making Markov Assumption : 
				\[
					P(w_n | w_1^{n-1}) \simeq P(w_n | w_{n-N+1}^{n-1})
				\]
				where $w_1^{n-1} = w_1 ... w_{n-1}$. This mean that for a n-gram, the next word probability depend only the last $N$ few words (or are approximate so). This approximation is the core of n-grams. 
				Bigram use only the last word to estimate the next, trigram the two lasts, and n-gram n-1 words.

				The kind of linguistic captured by bigram can be strictly syntactic, but also can be various kind of things, like cultural association, gender, etc. 

				\paragraph*{MLE as relative frequencies : } The estimation of the n-gram probabilities can be done using MLE and the \emph{Relative frequency} which is the ratio between observed frequence and the observed frequency of a prefix.
					\[
						P(w_n | w^{n-1}_{n-N+1}) = \frac{C(w^{n-1}_{n-N+1}w_n)}{C(w^{n-1}_{n-N+1})}
					\]
				\paragraph*{Use log space :} For computing the probability multiplication, log space is much more stable.
				\paragraph*{How to decide on n (of the n-gram) :} In practice bigram are never used, trigram are more common, but this can be larger with a consequent training dataset (larger n-gram require obviously larger dataset). More over it can be required context data (outside sentences data) for correct inference on the begining and ending of a sentence. The context data can be replaced with pseudo word as padding. 
			

				\todo{Google exemple ressource \url{https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html}}
		
		\subsection{Evaluation of language models}

			The best way to evaluate a model is to test how much it performs on a application regarding some baseline models. This is called \emph{extrinsic evaluation}. However running some complete systems can be expensive, so \emph{Intrinsic evaluation} can be done.

			The question that can be ask is "Does it prefer real sentences to ungrammatical ones ?"\\
			The measure the perplexity of an unseen corpus can give this answer\\
			The \emph{Perplexity} is the inverse probability on the test set, normalized with the number of words.\\
			for a word W : \todo{Add perplexity formulas}.\\
			The lower the perplexity is, the better our model is on the test set.
			The perplexity can be understand as the \emph{Weighted average branching factor} of a language, i.e the number of possible next words that can follow any word. 
		\subsection{Sparsity}

			N-gram rely on dataset, which have some various problems.
			\begin{itemize}
				\item Bias : Specific vocabulary, old corpus, etc
				\item Missing sequences, leading to 0 probabilities.
				\item Unable to contains new words
			\end{itemize}
			This lead to sparsity (lot of 0 probabilities) in the Probabilities given by the n-grams. This means that for a lot of sequences, there is not a lot of different possibility for the next words. A generative n-gram will often then directly copy the corpus sentences, This can be seen as a kind of "overfitting" in this tasks. Furthermore, if the corpus is not adapted to the tasks then there is fewer chance to get good results.\\
			Moreover, if any sequence in a test set has a 0 probabilities then the full perplexity of the set is untractable.\\
			Some techniques exists 

			\paragraph*{Unknown words :} Open vocabulary, Out of vocabulary (OOV), OOV rate. <UNK> pseudo word can be added, using vocabulary restriction, or frequency.
			\paragraph*{Smoothing}
			What can be done for known word which appear in an unknown context ? Smoothing (discounting) to avoid 0 probabilities.
			\begin{itemize}
				\item Add-1, juste Add 1 to the count for each sequence. This is named the \emph{Laplace Smoothing}. It does not perform well, but useful as baseline.
				\item Add-K, optimizing k on a devset. Usefull for classification, but not for language modeling.
			\end{itemize}
			
			\paragraph*{Back-off}
				Simply, when n-gram as probabilties 0, try with lesser n until finding none 0 proba. The proba can still be discounted when n is going lower. Using for exemple the Katz Backoff for discounting. Often the Katz is used with a Good-Turing Smoothing.

			\paragraph*{Interpolation}
				Same idea as Back-Off, but mix the probabilities of different size of n-gram with different weight, instead of only the largest one.

			All these methods may have hyperparameters, such as the weighing of discounting. Those can be optimized with a dedicated separate dataset, named \emph{held-out} corpus.

	\section{Structured prediction}

		All the models that have been seen before forget about some very important points of the language : his inherent structure. Of course they don't need it theory, but models are always difficult to train, and using the language structure can only be benefitial. The formalism of this structure is called \emph{Part-Of-Speach}

		\subsection{Part-Of-Speech tagging (POS)}

			POS tagging is the assignement to each word of a sentence of a classes/syntactic categories. This is really useful because POS reveal a lot about the role of a word and his neighbours. POS make possible to extract the subject of an action, which information is the main and which one is only background, etc. POS is really important in information extraction. But text never come with POS labels, which explain the need for tagging algorithms. 

			\paragraph*{Tagset : } 
				The tagset is the different syntactic categories that we want to use for a task, 	there is no unicity as it might depend of the task, the language, and it can be extend to any language, even theoritical language, if they have the good properties. However, in NLP task, as most research is about english, the following tags are the most common : Adjective, adverb, interjection, noun, proper noun, verb, punctuation, symbol (like math symbol), adposition, auxiliary verb, coordination conjonction, determinant, numeral, particule, pronoun, subordination conjonction and other. Actually this is a little more complicated than this, for exemple have a look to the Penn Treebank Part-Of-Speech tagset with 45 categories for English.

			\paragraph*{Baseline method :}
				Of course it is impossible to set for each word in a dictionnary a unique tag but the baseline method is simply to assign to each words its most frequent POS tag according to a dataset and to set all unknown word to NOUN. The results with the baseline is quite impressive, arround 90\% for any test set which are similar to the training set.

			\paragraph*{POS ambiguities}
				However the exeptions are the devil in this case. Indeed, the ambiguities are the most difficult to get, and the ones whith the highest probabilities to deeply change the meaning of a sentences...\\
				POS is really a disambiguation task. And if ambiguous word (wrt POS) are 15\% of the vocabulary, they represent more than half of the words use in text. 



		\subsection{Probabilistic POS tagging}

			The concept of probabilistic POS tagging is the following : given $W = w_1,w_2,...,w_n,$ we want to compute $P(T|W)$ with $T = t_1, t_2, ..., t_n$ a sequence of POS tags.\\
			Therefore, A generative approach can be made, with
			 \[P(T | W) \simeq P(W|T) P(T)\] 
			 and $P(T)$ can be decompose with the chain rule and simplify with markov assumption (The tag $t_i$ depends only on a few previous tags) 
			\[P(T) \simeq P(t_1)P(t_2 | t_1)...P(t_n | t_{n-1})\] 
			and $P(W |T)$ can be compute assuming that each word depend only on his tag (and not on any other words/tags) : 
			\[P(W|T) \simeq P(w_1 |t_1)P(w_2 |t_2)...P(w_n |t_n)\]
			and finally $P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_i)}{C(t_{i-1})}$ and $P(w_i | t_{i} )= \frac{C(w_{i}, t_i)}{C(t_{i})}$\\
			Then we can tag new sentences, going left to right, using at each step the highest $P(T | W)$ for tagging the next word. At each step it require to compute only $P(t_i | t_{i-1})P(w_i |t_i)$.\\
			However, using the max probabilities greedily make the algoritms to ignore potentially goods paths. These approach can't guarranty that we find $\hat{T} = \arg\max_T P(T |W)$.

		\subsection{Hidden Markov Model (HMM) tagger}

			HMM are sequence model (or classifier), which mean there are model which give an output for each element of the input sequence. 

			\todo[inline]{Add reference to a good definition and explanations of hidden markov model}

			In the POS Tagging task, the observable event are the words of the sentence, and the hidden event are the TAG for each words. The HMM has 2 probability matrices, one for the likelihood $P(t_i | t_{i-1})$ the transition probability and one for the likelihood $P(w_i | t_{i} )$ the emission probability. \todo{Add graph of HMM example}.\\
			The transition Matrices are estimate with the equation as before. 

			\paragraph*{Tagging as decoding : }
			Here is the important part, now that the HMM is set up with the same information (probabilities) of the simple probabilistic tagging the difference is the Tagging. Using a HMM, the tagging is actually the decoding task and with the Viterbi Algorithm it is possible to find the best sequence.

			\begin{algorithm}[H]
				\KwData{Sequence of word and HNN state graph}
				\KwResult{Best-path (i.e best sequence) and the according probability}
				N = len(state graph)\\
				T = len(Sequence)\\
				Create a path probability matrix viterbi[N, T]				
					\ForAll{sate s}
					{
						viterbi[s, 1] $\leftarrow \pi_s * b_s(o_1) =  \pi_s * B_{s, 1}$ = Initial probability distribution times Emission probability.\\
						backpointer[s, 1] $ \leftarrow 0$  
					}
					\ForAll{t in T}
					{
						\ForAll{state s}
						{
							viterbi[s, t] $\leftarrow \arg \max_{s'}$ viterbi[s', t-1] $* A_{s', s} * B_{s, t}$\\
							backpointer[s, t] $\leftarrow \max_{s'}$ viterbi[s', t-1] $* A_{s', s} * B_{s, t}$
						}
					}
				bestpathprob = $\max_s viterbi[s, T]$\\
				bestpathpointer = $\arg\max_s viterbi[s, T]$\\
				bestpath = path starting at best pointer\\
				return bestpath, bestpathprobb\\
				
				\caption{Viterbi Algorithm for finding optimal sequence of Tag}
			\end{algorithm}
		\subsection{Maximum Entropy Markov Model (MEMM) for POS tagging}

		\subsection{Other approachs}
				CRF

			\paragraph*{Performance}
				MEMM get 97\% against 98\% for human

			\paragraph*{Tools}
				Spacy, NLTK

	\section{Sequence Models}

		\paragraph*{Sequence Tagging}

		\subsection{Types of Sequence Models}

		\subsection{Neural Sequence Modelling}

				\paragraph*{Non-Markovian Assumption}

		\subsection{Language Modelling with Recursion}

	\section{Recurrent Neural Network}
		\subsection{Structure of LM}

		\subsection{Training of RNNLM}

				\paragraph*{Effective Back-Prop Strategies}

					-Back-Prop Through Time\\
					-Truncated Back-Prop Through Time

				\paragraph*{Vanishing Gradient}

		\subsection{Gated Recurrent Neural Network}

		\subsection{Other Variant}

				- Long Short-Term Memory networks (LSTM)
				- Similar functions
	\section{Bi-directional RNN}

	\section{Transformer Based Models}





		
% chapter nlp (end)