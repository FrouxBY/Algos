%!TEX root = ../master.tex
\chapter{Common Machine Learning algorithms}\label{ch:introduction}

This chapter is dedicated to the most common ML algorithms, a major part of the notes come from the mml-books.com \todo{Add bibtex reference}

It is intended to introduce different concept used in most Machine Learning Algorithms before moving to more specific algorithms.

\section{What is Machine Learning ?}

\todo{Add short intro that explain the goal, the process, etc of Machine Learning}
\section{Graphical Model for Probabilistic Inference}

Based on the Probabilistic Inference Course of Marc Deisenroth (Imperial College)

	\subsection{Probabilistic Pipeline}

	Here is a simple pipeline of the inference process with a model

		\figcap{figures/pipeline_proba}{}{Simple Pipeline of how to build model for inference}{0.8}

	\subsection{Probabilistic graphical Model}

		In order to deal with complex and big probalistic model, we can use different kind of graphs which represent relationships between random variables. We define the random variables as nodes and the probabiistic or functional relationship between variables as edges in the graphs. With them you can :
		\begin{itemize}
			\item Visualize the structure
			\item Have insights into properties (such as conditional independance)
			\item Design or Motivate new models
			\item Compute some inference and learning as graphical manipulations
		\end{itemize}

		There exists 3 kinds of model : Bayesian networks (directed graphical models), Markov random fields (undirected graphical models) and factor graphs (with nodes which are not random variables)/

		\subsubsection{Bayesian Networks}

			\todo[inline]{add basic graphs exemple}

			They are defined by 
			\begin{itemize}
				\item Nodes : Random variables
				\item Shaded nodes: Observed random variables
				\item Other : Latent Variables
				\item Edges : (a, b) $\iff$ conditionnal distribution $p(b|a)$
			\end{itemize}

			\todo[inline]{add graphicals model of linear regression}

			\paragraph*{D-Separation:} A set A of nodes is d-Separated (conditionnaly independant) from B by C iff all path between A and B are blocked. C is tht set of observed variables in the graphical model.
			\begin{definition}
				A Path is \emph{Blocked} id it includes a node such that either :
				\begin{itemize}
					\item The arrow on the path meet either head-to-tail or tail-to-tail at the node, and the node is in C 
					\item The arrow meet head-to-head at the node, and neither the node nor any of its descendants is in the set C
				\end{itemize}
			\end{definition}

			\paragraph*{Markov Random Fields}
				Defined by : if X and Y are not connected directly, then X and Y are conditionnaly independant given everything else.

				Then Joint distributions are the product of cliques in the graphs, and the full joint (of all variables) is the product of the joint of the maximum cliques

				\[
					p(x) = fra{1}{Z} \Pi _C \psi _C(x_C)
				\]
				Where C describe a maximal clique, $x_C$ all the variable in C, $\psi_C$ the clique potential and Z a normalisation constant. 

				Check for the condiational independance : Check if when remmoving the observable variable there is no path between two set of random variable.

				The potentials can be seen as Energy Functions, which mean we can write $\psi_C(x_C) = exp(-E(x_C))$ with E some energy functions, and then $-log(p(x)) = \sum_C E(x_C)$ is the sum of energies of the clique potential.


				\todo[inline]{Add Image Denoising exemple}

			\paragraph*{Factor Graphs}

				(Un)directed graphical models express a global function of several variables as a product of factors over subsets of those variables

				Factor graphs make this decomposition explicit by introducing additional nodes for the factors themselves


\section{Linear Regression}

	The Linear regression problem correspond to find a linear mapping $f(x)$ based on noisy observation $y = f(x) + \epsilon$, where $\epsilon$ is a noise. 
	Finding the regression function require : 
	\begin{itemize}
		\item Choice of parameters (function classes, dimension)
		\item Choice of probabilistic model (Loss function, ...)
		\item Avoiding under and overfitting
		\item Modeling uncertainty on data
	\end{itemize}

	\begin{definition}
			\begin{itemize}
				\item $p(x, y)$ is te joint distribution
				\item $p(x)$ and $p(y)$ are the marginal distributions
				\item $p(y|x)$ is the conditional distribution of $y$ given $x$
			 	\item in the context of regression, $p(y|x)$ is called likelihood, $p(x|y)$ the posterior, $p(x)$ the prior and $p(y)$ the marginal likelihood or evidence.
			 	\item they are related by the Bayes' Theorem : $p(x|y) = \frac{p(y|x)p(x)}{p(y)}$ 
			\end{itemize}
		\end{definition}

	\subsection{Conjugacy} % (fold)
	\label{sub:conjugacy}
		In order to compute the posterior, we require some calculations which imply the prior, and can be intractable as a closed form. But given a likelihood, it can exist prior which give closed-form solution for the posterior. This is the principle of conjugacy (between the likelihood and the prior)
	% subsection conjugacy (end)

	\subsection{Maximum Likelihood Estimation (MLE)}
	In order to obtain the parameters, we need to choose a quantity to optimize. The first and most common idea is to choose the Likelihood as a cost function to optimize. The seems natural because the likelihood represent the probability given the $X$ variables to obtain the observed variables $Y$ and we want this to be maximum on the known data (which are the training set).

	\[
		\theta^* = \arg \max_{\theta} p(y | X, \theta)
	\]

	To find the desired parameters that maximize the likelihood, we typically do gradient ascent (or gradient descent on the negative likelihood). For numerical reasons, we apply the log-transformation to the probleme and minimize the negative log-likelihood. 

		\paragraph*{Closed-Form Solution - Linear Regression}

			In some cases, a closed-form solution exist, which make computation easy (but not necesseraly cheap)

		\paragraph*{MLE with Features}

	\subsection{Overfitting}

	\subsection{Regularization}
	\subsection{Maximum A Posteriori Estimation (MAP)}

		\paragraph*{MAP for Linear Regression}

\section{Gradient Descent}

	\subsection{Simple Gradient Descent}

	\subsection{Gradient Descent with Momentum}

	\subsection{Stochastic Gradient Descent}


\section{Model Selection and Validation}

	\subsection{Cross-Validation}

	\subsection{Bayesian Model Selection}
		\paragraph*{Bayes Factor}

		\paragraph*{Fully Basyesian Treatment}

	\subsection{Marginal Likelihood}

\section{Bayesian Linear Regression}

	\subsection{Mean and Variance}

	\subsection{Sample function}

\section{Features Extraction}
	\todo{Simply explain and redirect to relevant chapter (dim reduc, computer vision, ...)}

\section{Support Vector Machine}

	\subsection{Linear Separating Hyperplane}

		\paragraph*{Lagrangian duality}
		\paragraph*{Karush-Kuhn-Tucker Condition for Optimality}
	\subsection{SVM dual problem}

	\subsection{Support Vector Regression}





